{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids =  tensor([[30355,    15,  1566,    12,  2379,    10,    37,   629,    19,   786,\n",
      "             5,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "attention_mask =  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])\n",
      "labels =  tensor([[ 325, 4053,  259, 6030,    5,    1,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]])\n",
      "No using toekn -100 loss =  tensor(13.8850, grad_fn=<NllLossBackward0>)\n",
      "output_ids =  tensor([[ 325, 4053,  259, 6030,    5,    1,    1,    3,  325,  325,  325,  325,\n",
      "          325,  325,  325,  325,  325,  325,  325,  325,  325,  325,  325,  325,\n",
      "          325,  325,  325,  325,  325,  325,  325,  325,  325,  325,  325,  325,\n",
      "          325,  325,  325,  325,  325,  325,  325,  325,  325,  325,  325,  325,\n",
      "          325,  325]])\n",
      "tokenized_output_ids =  ['La maison est belle.  La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La La']\n",
      "output_ids_remove_after_eos =  [tensor([ 325, 4053,  259, 6030,    5])]\n",
      "tokenized_output_ids_remove_after_eos =  ['La maison est belle.']\n",
      "_100_labels =  tensor([[ 325, 4053,  259, 6030,    5,    1, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100]])\n",
      "Using token -100, loss =  tensor(0.2430, grad_fn=<NllLossBackward0>)\n",
      "model_generate =  tensor([[   0,  325, 4053,  259, 6030,    5,    1]])\n",
      "['La maison est belle.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load mô hình T5 và tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Văn bản đầu vào và đầu ra\n",
    "input_text = \"Translate English to French: The house is beautiful.\"\n",
    "target_text = \"La maison est belle.\"\n",
    "\n",
    "# Mã hóa đầu vào và đầu ra\n",
    "input_encodings = tokenizer(input_text, return_tensors=\"pt\", max_length = 50, padding=\"max_length\", truncation=True)\n",
    "target_encodings = tokenizer(target_text, return_tensors=\"pt\",max_length = 50, padding=\"max_length\", truncation=True)\n",
    "\n",
    "input_ids = input_encodings[\"input_ids\"]\n",
    "attention_mask = input_encodings[\"attention_mask\"]\n",
    "labels = target_encodings[\"input_ids\"]\n",
    "print(\"input_ids = \", input_ids)\n",
    "print(\"attention_mask = \", attention_mask)\n",
    "print(\"labels = \", labels)\n",
    "\n",
    "outputs = model(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        labels=labels,\n",
    "        )\n",
    "loss = outputs.loss\n",
    "print(\"No using toekn -100 loss = \", loss)\n",
    "\n",
    "logits = outputs.logits\n",
    "output_ids = torch.argmax(logits, dim=-1)\n",
    "print(\"output_ids = \", output_ids)\n",
    "tokenized_output_ids = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "print('tokenized_output_ids = ', tokenized_output_ids)\n",
    "\n",
    "def remove_after_eos(output_ids):\n",
    "    cleaned_output = []\n",
    "    for sequence in output_ids:\n",
    "        eos_position = (sequence == 1).nonzero(as_tuple=True)[0]\n",
    "        if len(eos_position) > 0:  # Nếu tìm thấy <EOS>\n",
    "            cleaned_output.append(sequence[: eos_position[0]])  # Giữ lại từ đầu đến <EOS>\n",
    "        else:\n",
    "            cleaned_output.append(sequence)  # Không có <EOS>, giữ nguyên\n",
    "    return cleaned_output\n",
    "\n",
    "output_ids_remove_after_eos = remove_after_eos(output_ids)\n",
    "print(\"output_ids_remove_after_eos = \", output_ids_remove_after_eos)\n",
    "\n",
    "tokenized_output_ids_remove_after_eos = tokenizer.batch_decode(output_ids_remove_after_eos, skip_special_tokens=True)\n",
    "print(\"tokenized_output_ids_remove_after_eos = \", tokenized_output_ids_remove_after_eos)\n",
    "#############################################################\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "print(\"_100_labels = \", labels)\n",
    "outputs = model(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        labels=labels,\n",
    "        )\n",
    "loss = outputs.loss\n",
    "print(\"Using token -100, loss = \", loss)\n",
    "#############################################################\n",
    "model_generate = model.generate(input_ids, max_length=512)\n",
    "print('model_generate = ', model_generate)\n",
    "print(tokenizer.batch_decode(model_generate, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([703,  75, 117, 703,  75,   1])\n",
      "abc; abc\n"
     ]
    }
   ],
   "source": [
    "input_text = \"abc; abc\"\n",
    "token = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "print(token)\n",
    "print(tokenizer.decode(token, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4}\n",
      "{2, 3, 4, 5}\n",
      "{2, 3, 4}\n",
      "{1}\n",
      "{5}\n"
     ]
    }
   ],
   "source": [
    "a = set([1, 2, 3, 4])\n",
    "b = set([2, 3, 4, 5])\n",
    "print(a)\n",
    "print(b)\n",
    "print(a & b)\n",
    "print(a - b)\n",
    "print(b - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    a = 10\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"/chain-of-thought-ABSA/results/experiment_6/{epoch + 1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model.state_dict(), f\"/chain-of-thought-ABSA/results/experiment_6/{1}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
