{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xây dựng tập huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng tập real_train_data\n",
    "import json\n",
    "\n",
    "# Đọc dữ liệu từ file JSON\n",
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Tạo các tác vụ\n",
    "def create_tasks(data):\n",
    "    task_data = {}\n",
    "    for i, sample in enumerate(data):  # Sử dụng chỉ số i để phân biệt các câu\n",
    "        aspect_opinion_sentiments = []\n",
    "        for index in range(len(sample[\"aspects\"])):\n",
    "            aspect = \" \".join(sample[\"aspects\"][index][\"term\"])\n",
    "            opinion = \" \".join(sample[\"opinions\"][index][\"term\"])\n",
    "            sentiment = sample[\"aspects\"][index][\"polarity\"]\n",
    "            aspect_opinion_sentiments.append((aspect, opinion, sentiment))\n",
    "\n",
    "        aspects = \"; \".join(sorted([aspect for aspect, _, _ in aspect_opinion_sentiments]))\n",
    "        opinions = \"; \".join(sorted([opinion for _, opinion, _ in aspect_opinion_sentiments]))\n",
    "        aspect_opinions = \"; \".join(sorted([f\"{aspect}, {opinion}\" for aspect, opinion, _ in aspect_opinion_sentiments]))\n",
    "        aspect_opinion_sentiments = \"; \".join(sorted([f\"{aspect}, {opinion}, {sentiment}\" for aspect, opinion, sentiment in aspect_opinion_sentiments]))\n",
    "        raw_text = sample[\"raw_words\"]\n",
    "        # Tác vụ 1: I -> A\n",
    "        task_data[f\"{i}_1\"] = {\n",
    "            \"id\": i,\n",
    "            \"task\": 1,\n",
    "            \"text_input\": f\"Task 1, predict aspect terms\\nInput: {raw_text}\",\n",
    "            \"text_label\": aspects,\n",
    "            \"text_predict\": aspects,\n",
    "            \"text_sentence\": raw_text,\n",
    "        }\n",
    "        # Tác vụ 2: I -> O\n",
    "        task_data[f\"{i}_2\"] = {\n",
    "            \"id\": i,\n",
    "            \"task\": 2,\n",
    "            \"text_input\": f\"Task 2, predict opinion terms\\nInput: {raw_text}\",\n",
    "            \"text_label\": opinions,\n",
    "            \"text_predict\": opinions,\n",
    "            \"text_sentence\": raw_text,\n",
    "        }\n",
    "        # Tác vụ 3: I, A -> A-O\n",
    "        task_data[f\"{i}_3\"] = {\n",
    "            \"id\": i,\n",
    "            \"task\": 3,\n",
    "            \"text_input\": f\"Task 3, predict aspect term - opinion term sets with these aspect terms\\nInput: {raw_text}\\nAspect terms: {aspects}\",\n",
    "            \"text_label\": aspect_opinions,\n",
    "            \"text_predict\": aspect_opinions,\n",
    "            \"text_sentence\": raw_text,\n",
    "        }\n",
    "        # Tác vụ 4: I, O -> A-O\n",
    "        task_data[f\"{i}_4\"] = {\n",
    "            \"id\": i,\n",
    "            \"task\": 4,\n",
    "            \"text_input\": f\"Task 4, predict aspect term - opinion term sets with these opinion terms\\nInput: {raw_text}\\nOpinion terms: {opinions}\",\n",
    "            \"text_label\": aspect_opinions,\n",
    "            \"text_predict\": aspect_opinions,\n",
    "            \"text_sentence\": raw_text,\n",
    "        }\n",
    "        # Tác vụ 5: I, A, O -> A-O\n",
    "        task_data[f\"{i}_5\"] = {\n",
    "            \"id\": i,\n",
    "            \"task\": 5,\n",
    "            \"text_input\": f\"Task 5, predict aspect term - opinion term sets with these aspect terms and opinion terms\\nInput: {raw_text}\\nAspect terms: {aspects}\\nOpinion terms: {opinions}\",\n",
    "            \"text_label\": aspect_opinions,\n",
    "            \"text_predict\": aspect_opinions,\n",
    "            \"text_sentence\": raw_text,\n",
    "        }\n",
    "        # Tác vụ 6: I, A-O, A-O, A-O -> A-O\n",
    "        task_data[f\"{i}_6\"] = {\n",
    "            \"id\": i,\n",
    "            \"task\": 6,\n",
    "            \"text_input\": f\"Task 6, predict aspect term - opinion term sets with results of task 3, 4, 5\\nInput: {raw_text}\\nTask 3 results: {aspect_opinions}\\nTask 4 results: {aspect_opinions}\\nTask 5 results: {aspect_opinions}\",\n",
    "            \"text_label\": aspect_opinions,\n",
    "            \"text_predict\": aspect_opinions,\n",
    "            \"text_sentence\": raw_text,\n",
    "        }\n",
    "        # Tác vụ 7: I, A-O -> A-O-S\n",
    "        task_data[f\"{i}_7\"] = {\n",
    "            \"id\": i,\n",
    "            \"task\": 7,\n",
    "            \"text_input\": f\"Task 7, predict aspect term - opinion term - sentiment polarity sets with these aspect term - opinion term sets\\nInput: {raw_text}\\nAspect term - opinion term sets: {aspect_opinions}\",\n",
    "            \"text_label\": aspect_opinion_sentiments,\n",
    "            \"text_predict\": aspect_opinion_sentiments,\n",
    "            \"text_sentence\": raw_text,\n",
    "        }\n",
    "    return task_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch_keys, train_tasks, q, tokenizer):\n",
    "    text_inputs, text_labels = [], []\n",
    "    for key in batch_keys:\n",
    "        item_data = train_tasks[key]\n",
    "\n",
    "        id = item_data[\"id\"]\n",
    "        task = item_data['task']\n",
    "        text_input = item_data[\"text_input\"]\n",
    "        text_label = item_data[\"text_label\"]\n",
    "        # text_predict = item_data[\"text_predict\"]\n",
    "        text_sentence = item_data[\"text_sentence\"]\n",
    "\n",
    "        # Task 3: Sử dụng predict từ task1\n",
    "        if task == 3 and random.random() < q:\n",
    "            aspects = train_tasks[f\"{id}_1\"][\"text_predict\"]\n",
    "            text_input = f\"Task 3, predict aspect term - opinion term sets with these aspect terms\\nInput: {text_sentence}\\nAspect terms: {aspects}\"\n",
    "        # Task 4: Sử dụng predict từ task2\n",
    "        if task == 4 and random.random() < q:\n",
    "            opinions = train_tasks[f\"{id}_2\"][\"text_predict\"]\n",
    "            text_input = f\"Task 4, predict aspect term - opinion term sets with these opinion terms\\nInput: {text_sentence}\\nOpinion terms: {opinions}\"\n",
    "        # Task 5: Sử dụng predict từ task1 và task2\n",
    "        if task == 5:\n",
    "            aspects = train_tasks[f\"{id}_1\"][\"text_predict\"] if random.random() < q else train_tasks[f\"{id}_1\"][\"text_label\"]\n",
    "            opinions = train_tasks[f\"{id}_2\"][\"text_predict\"] if random.random() < q else train_tasks[f\"{id}_2\"][\"text_label\"]\n",
    "            text_input = f\"Task 5, predict aspect term - opinion term sets with these aspect terms and opinion terms\\nInput: {text_sentence}\\nAspect terms: {aspects}\\nOpinion terms: {opinions}\"\n",
    "        # Task 6: Sử dụng predict từ task3, task4, task5\n",
    "        if task == 6:\n",
    "            result_3 = train_tasks[f\"{id}_3\"][\"text_predict\"] if random.random() < q else train_tasks[f\"{id}_3\"][\"text_label\"]\n",
    "            result_4 = train_tasks[f\"{id}_4\"][\"text_predict\"] if random.random() < q else train_tasks[f\"{id}_4\"][\"text_label\"]\n",
    "            result_5 = train_tasks[f\"{id}_5\"][\"text_predict\"] if random.random() < q else train_tasks[f\"{id}_5\"][\"text_label\"]\n",
    "            text_input = f\"Task 6, predict aspect term - opinion term sets with results of task 3, 4, 5\\nInput: {text_sentence}\\nTask 3 results: {result_3}\\nTask 4 results: {result_4}\\nTask 5 results: {result_5}\"\n",
    "        # Task 7: Sử dụng predict từ task6\n",
    "        if task == 7 and random.random() < q:\n",
    "            aspect_opinions = train_tasks[f\"{id}_6\"][\"text_predict\"]\n",
    "            text_input = f\"Task 7, predict aspect term - opinion term - sentiment polarity sets with these aspect term - opinion term sets\\nInput: {text_sentence}\\nAspect term - opinion term sets: {aspect_opinions}\"\n",
    "        \n",
    "        # Tokenize inputs and labels\n",
    "        text_inputs.append(text_input)\n",
    "        text_labels.append(text_label)\n",
    "    \n",
    "    # Tokenize the entire batch\n",
    "    tokenized_inputs = tokenizer(text_inputs, padding=True, return_tensors=\"pt\")\n",
    "    tokenized_labels = tokenizer(text_labels, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Return the batch as a dictionary\n",
    "    return {\n",
    "        \"batch_keys\": batch_keys,\n",
    "        \"input_ids\": tokenized_inputs[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "        \"labels\": tokenized_labels[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "def build_dataloader(train_tasks, q, batch_size, tokenizer):\n",
    "    # Tạo danh sách các key để chia batch\n",
    "    task_keys = list(train_tasks.keys())\n",
    "    # Tạo DataLoader\n",
    "    return DataLoader(\n",
    "        task_keys,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,  # Shuffle để tăng tính ngẫu nhiên\n",
    "        collate_fn=lambda batch_keys: collate_fn(batch_keys=batch_keys, train_tasks=train_tasks, q=q, tokenizer=tokenizer)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xử lý tập validation và đánh giá mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# === HÀM ĐỂ CHẠY SUY LUẬN TRÊN BATCH ===\n",
    "def batch_predict(model, tokenizer, inputs, device):\n",
    "    encoded_inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True).to(device)\n",
    "    outputs = model.generate(**encoded_inputs, max_length=100)\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "def evaluate_pipeline_batch(model, tokenizer, validation_data, device, batch_size):\n",
    "    # Initialize counters\n",
    "    count_true_positive = 0\n",
    "    count_false_positive = 0\n",
    "    count_false_negative = 0\n",
    "    \n",
    "    # Divide validation data into batches\n",
    "    for i in range(0, len(validation_data), batch_size):\n",
    "        batch = validation_data[i:i + batch_size]\n",
    "        raw_texts = [sample[\"text_input\"] for sample in batch]\n",
    "        ground_truth_batch = [sample[\"text_label\"] for sample in batch]\n",
    "\n",
    "        # Step 1: Task 1 (I -> A)\n",
    "        task_1_inputs = [\n",
    "            f\"Task 1, predict aspect terms\\nInput: {text}\"\n",
    "            for text in raw_texts]\n",
    "        task_1_outputs = batch_predict(model, tokenizer, task_1_inputs, device)\n",
    "        # print(\"task_1_inputs = \", task_1_inputs)\n",
    "        # print(\"task_1_outputs = \", task_1_outputs)\n",
    "        # Step 2: Task 2 (I -> O)\n",
    "        task_2_inputs = [\n",
    "            f\"Task 2, predict opinion terms\\nInput: {text}\" \n",
    "            for text in raw_texts]\n",
    "        task_2_outputs = batch_predict(model, tokenizer, task_2_inputs, device)\n",
    "        # print(\"task_2_inputs = \", task_2_inputs)\n",
    "        # print(\"task_2_outputs = \", task_2_outputs)\n",
    "        # Step 3: Task 3 (I, A -> A-O)\n",
    "        task_3_inputs = [\n",
    "            f\"Task 3, predict aspect term - opinion term sets with these aspect terms\\nInput: {text}\\nAspect terms: {aspect}\"\n",
    "            for text, aspect in zip(raw_texts, task_1_outputs)\n",
    "        ]\n",
    "        task_3_outputs = batch_predict(model, tokenizer, task_3_inputs, device)\n",
    "        # print(\"task_3_inputs = \", task_3_inputs)\n",
    "        # print(\"task_3_outputs = \", task_3_outputs)\n",
    "        # Step 4: Task 4 (I, O -> A-O)\n",
    "        task_4_inputs = [\n",
    "            f\"Task 4, predict aspect term - opinion term sets with these opinion terms\\nInput: {text}\\nOpinion terms: {opinion}\"\n",
    "            for text, opinion in zip(raw_texts, task_2_outputs)\n",
    "        ]\n",
    "        task_4_outputs = batch_predict(model, tokenizer, task_4_inputs, device)\n",
    "        # print(\"task_4_inputs = \", task_4_inputs)\n",
    "        # print(\"task_4_outputs = \", task_4_outputs)\n",
    "        # Step 5: Task 5 (I, A, O -> A-O)\n",
    "        task_5_inputs = [\n",
    "            f\"Task 5, predict aspect term - opinion term sets with these aspect terms and opinion terms\\nInput: {text}\\nAspect terms: {aspect}\\nOpinion terms: {opinion}\"\n",
    "            for text, aspect, opinion in zip(raw_texts, task_1_outputs, task_2_outputs)\n",
    "        ]\n",
    "        task_5_outputs = batch_predict(model, tokenizer, task_5_inputs, device)\n",
    "        # print(\"task_5_inputs = \", task_5_inputs)\n",
    "        # print(\"task_5_outputs = \", task_5_outputs)\n",
    "        # Step 6: Task 6 (I, A-O -> A-O)\n",
    "        task_6_inputs = [\n",
    "            f\"Task 6, predict aspect term - opinion term sets with results of task 3, 4, 5\\nInput: {text}\\nTask 3 results: {task_3}\\nTask 4 results: {task_4}\\nTask 5 results: {task_5}\"\n",
    "            for text, task_3, task_4, task_5 in zip(raw_texts, task_3_outputs, task_4_outputs, task_5_outputs)\n",
    "        ]\n",
    "        task_6_outputs = batch_predict(model, tokenizer, task_6_inputs, device)\n",
    "        # print(\"task_6_inputs = \", task_6_inputs)\n",
    "        # print(\"task_6_outputs = \", task_6_outputs)\n",
    "        # Step 7: Task 7 (I, A-O -> A-O-S)\n",
    "        task_7_inputs = [\n",
    "            f\"Task 7, predict aspect term - opinion term - sentiment polarity sets with these aspect term - opinion term sets\\nInput: {text}\\nAspect term - opinion term sets: {task_6_output}\"\n",
    "            for text, task_6_output in zip(raw_texts, task_6_outputs)\n",
    "        ]\n",
    "        task_7_outputs = batch_predict(model, tokenizer, task_7_inputs, device)\n",
    "        # print(\"task_7_inputs = \", task_7_inputs)\n",
    "        # print(\"task_7_outputs = \", task_7_outputs)\n",
    "        # print(\"ground_truth_batch = \", ground_truth_batch)\n",
    "        # return 0, 0, 0\n",
    "        # Evaluate predictions\n",
    "        for predicted, true in zip(task_7_outputs, ground_truth_batch):\n",
    "            predicted_set = set(predicted.split('; '))  # Split predictions into sets of tuples\n",
    "            true_set = set(true.split('; '))  # Split ground truth into sets of tuples\n",
    "            # Calculate true positives, false positives, and false negatives\n",
    "            count_true_positive += len(predicted_set & true_set)\n",
    "            count_false_positive += len(predicted_set - true_set)\n",
    "            count_false_negative += len(true_set - predicted_set)\n",
    "    \n",
    "    # Compute Precision, Recall, and F1-score\n",
    "    precision = count_true_positive / (count_true_positive + count_false_positive) if count_true_positive + count_false_positive > 0 else 0\n",
    "    recall = count_true_positive / (count_true_positive + count_false_negative) if count_true_positive + count_false_negative > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# === HÀM XỬ LÝ VALIDATION DATA ===\n",
    "def process_validation_data(validation_file):\n",
    "    with open(validation_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    processed_data = []\n",
    "    for sample in data:\n",
    "        aspect_opinion_sentiments = []\n",
    "        for index in range(len(sample[\"aspects\"])):\n",
    "            aspect = \" \".join(sample[\"aspects\"][index][\"term\"])\n",
    "            opinion = \" \".join(sample[\"opinions\"][index][\"term\"])\n",
    "            sentiment = sample[\"aspects\"][index][\"polarity\"]\n",
    "            aspect_opinion_sentiments.append((aspect, opinion, sentiment))\n",
    "        # Add processed sample\n",
    "        processed_data.append({\n",
    "            \"text_input\": sample[\"raw_words\"],\n",
    "            \"text_label\": \"; \".join(sorted([f\"{aspect}, {opinion}, {sentiment}\" for aspect, opinion, sentiment in aspect_opinion_sentiments]))\n",
    "        })\n",
    "    # random.shuffle(processed_data)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_455022/3259633818.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/chain-of-thought-ABSA/results/32.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5782122905027933, 0.6, 0.5889046941678521)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "validation_data = process_validation_data(\"/chain-of-thought-ABSA/Dataset/SemEval14/Validation/Laptops_Opinion_Validation.json\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.load_state_dict(torch.load(\"/chain-of-thought-ABSA/results/32.pt\"))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "evaluate_pipeline_batch(\n",
    "    model=model, \n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\"), \n",
    "    validation_data = validation_data, \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), \n",
    "    batch_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW\n",
    "# Hàm huấn luyện\n",
    "def train_model(model, tokenizer, train_tasks, validation_data, num_epochs, batch_size, lr, q_sped):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    q = 0\n",
    "    dataloader = build_dataloader(train_tasks=train_tasks, q = q, batch_size=batch_size, tokenizer=tokenizer)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    previous_f1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        #Huấn luyện\n",
    "        print(\"Training\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch_keys = batch[\"batch_keys\"]\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"].to(device), \n",
    "                attention_mask=batch[\"attention_mask\"].to(device), \n",
    "                labels=batch[\"labels\"].to(device))\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # add loss\n",
    "            total_loss += loss.item()\n",
    "            # Lưu dự đoán vào train_tasks nếu random.random() < p\n",
    "            if random.random() < (1 - q):\n",
    "                for key, prediction in zip(batch_keys, tokenizer.batch_decode(torch.argmax(logits, dim=-1), skip_special_tokens=True)):\n",
    "                    train_tasks[key][\"text_predict\"] = prediction\n",
    "        \n",
    "        # Log loss mỗi epoch\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1} finished with loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Đánh giá\n",
    "        print(\"Evaluating\")\n",
    "        model.eval()\n",
    "        precision, recall, f1 = evaluate_pipeline_batch(model, tokenizer, validation_data, device, batch_size = batch_size)\n",
    "        print(precision, recall, f1)\n",
    "        if(f1 > previous_f1):\n",
    "            previous_f1 = f1\n",
    "            torch.save(model.state_dict(), f\"/chain-of-thought-ABSA/results/{epoch + 1}.pt\")\n",
    "        else:\n",
    "            if (q >= 1):\n",
    "                return\n",
    "            previous_f1 = 0\n",
    "            q+=q_sped\n",
    "            print(q)\n",
    "            dataloader = build_dataloader(train_tasks=train_tasks, q = q, batch_size=batch_size, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo tập huấn luyện\n",
    "train_data = load_data(\"/chain-of-thought-ABSA/Dataset/SemEval14/Train/Laptops_Opinion_Train.json\")\n",
    "train_tasks = create_tasks(train_data)\n",
    "# tạo tập validation\n",
    "validation_data = process_validation_data(\"/chain-of-thought-ABSA/Dataset/SemEval14/Validation/Laptops_Opinion_Validation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "train_model(\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\"), \n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\"), \n",
    "    train_tasks = train_tasks,\n",
    "    validation_data = validation_data,\n",
    "    num_epochs = 1000,\n",
    "    batch_size = 16, \n",
    "    lr = 3e-5,\n",
    "    q_sped = 0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
